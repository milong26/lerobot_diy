> 我再也不搞merge了因为auto-merge真的把我当傻子

# install1
1. https://github.com/milong26/lerobot_diy.git  
2. conda create -y -n lerobot python=3.10 （需要安装conda,可以安装[miniconda](https://www.anaconda.com/docs/getting-started/miniconda/install#linux-2)）

在lerobot-diy目录下   pip install -e ".[feetech,smolvla,async]"


When using miniconda, install ffmpeg in your environment:

`conda install ffmpeg -c conda-forge`


3. cuda环境和pytorch环境最好一致，后来因为找不到cuda12.0用的pytorch所以改成pytorch11.9+cuda11.8，反正都是安装 pytorch
4. 准备模型 参考models 需要安装好[large file](https://github.com/git-lfs/git-lfs/releases) git lfs pull
5. 准备training_dataset：async上传

pip install pyrealsense2 如果需要depth image处理的话


# collect
1. port一般没动，需要`sudo chmod 666 /dev/ttyACM0` 0，1，2都要sudo，2是力传感器
2. 找opencv相机 `PYTHONPATH=src python -m lerobot.find_cameras` 一般也不用改，opencv不是6就是7，再找不到才用这个

## usage
先改好代码，或者直接用这一版的代码。

1. 收集：yaml文件提前写好 `PYTHONPATH=src python -m lerobot.record --config_path=simplify_work/work/collect/collect_data_template.yaml`
2. visualize数据集：`PYTHONPATH=src  python -m lerobot.scripts.visualize_dataset --repo-id=haello/1 --root=/home/qwe/.cache/huggingface/lerobot/test/collect_1 --episode-index=0`
3. 上传到服务器 `rsync -avz  fcam1/all150/ user@xx.xx.xx.xx:location in server`

## modified
### motor
有个小bug，没法读电机，改so100_follower_end_effector，新增num_retry=3`obs_dict = self.bus.sync_read("Present_Position",num_retry=3)`

### realsense camera
depth转深度图并保存

1. camera_pyrealsense.py 收集 需要有一个可以读color_image和depth_image的函数

    ```python
    def async_read_combined(self, timeout_ms: float = 200) -> tuple[np.ndarray, np.ndarray]:
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        if self.thread is None or not self.thread.is_alive():
            self._start_read_thread()

        if not self.new_frame_event.wait(timeout=timeout_ms / 1000.0):
            raise TimeoutError(f"Timed out waiting for frame from {self}")

        with self.frame_lock:
            color = self.latest_frame
            self.new_frame_event.clear()

        with self.depth_frame_lock:
            depth = self.latest_depth_frame
            self.new_depth_frame_event.clear()
        if self.use_depth:
            if color is None or depth is None :
                raise RuntimeError(f"{self}: Frame data incomplete")
        if self.use_depth:
            return color, depth
        else:
            return color, None
    ```
2. so100_follower.py 设备端
   1. make_force_sensor
   2. 启动、采集、关闭
3. force feature :dataset/utils
4. visualize_utils： 遇到observation.force暂时不绘制
5. record程序里面需要导入！就算不用也要导入
6. 新增：深度图保存改成16bit信息存储到两个通道，读的时候也不能直接从视频读（train的部分说明）
```python
    def encode_depth_to_rgb(self,depth_uint16):
        """将16位深度编码为RGB三通道图像"""
        # 确保depth_uint16是uint16格式
        depth_uint16 = depth_uint16.astype(np.uint16)

        # 拆分高8位和低8位
        r = (depth_uint16 >> 8).astype(np.uint8)
        g = (depth_uint16 & 0xFF).astype(np.uint8)
        b = np.zeros_like(r, dtype=np.uint8)  # B通道置0
        # 合成3通道图像
        rgb_image = cv2.merge((r, g, b))  # OpenCV用BGR顺序，注意这里为了方便读取，放成B,G,R
        return rgb_image
```

### force
1. 新增了一个repo
2. types里面增加FeatureType force
3. utils里面force的feature
4. record的时候给传感器归零
5. 目前force是保存为(15,1)的float64，应该是x0,y0,z0,x1,y1,z1,x2,...的形式。z可以看作是法向量的力的大小，xy需要用增量表示切向量。
6. 目前模型没有处理force特征。所以evaluate的时候不要设置，收集的时候倒是全收集了。

### dataset
1. lerobotdataset里面新增一个确定（/不）保存images文件夹的。用dataset.save_image_folder控制。默认为False（也就是删除。）
   ```python
    # 为了使用深度图,不删除
    if self.save_image_folder == False:
        shutil.rmtree(img_dir)
   ```

# train

## usage
1. 纯服务器调用：上传数据集后，在服务器端运行`nohup env CUDA_VISIBLE_DEVICES=1 HF_HUB_OFFLINE=1 PYTHONPATH=src   python -m lerobot.scripts.train   --config_path=simplify_work/work/train/fine_tune_0713_first100_depth.yaml   --policy.path=models/forsmolvla/smolvla_base   --policy.repo_id=lerobot/smolvla_base   > train_100_depth.log 2>&1 &`
2. 查看日志 `tail -f log.file`
3. 参数 configs/train.py里面，加了三个
    ```python
    # 为了在多模态里面选择用/不用depth_image，默认为false。scripts/train.py
    use_depth_image: bool=False
    use_force: bool=False
    use_language_tip: bool=False
    ```
4. 本地小测试`HF_HUB_OFFLINE=1 PYTHONPATH=src   python -m lerobot.scripts.train   --config_path=simplify_work/work/train/local_test.yaml   --policy.path=models/forsmolvla/smolvla_base   --policy.repo_id=lerobot/smolvla_base`

## modified
scripts/train.py里面

1. 修改tokenizer和multi gpu bug,os的指定改到命令行里面了

```python
# https://github.com/huggingface/lerobot/issues/1377
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
```
2. 根据config的3个变量，filter batch的内容，因为原始数据集里面feature比较多

```python
class FilteredBatchLoader:
    def __init__(self, dataloader, exclude_keys: list):
        self.dataloader = dataloader
        self.exclude_keys = set(exclude_keys)

    def __iter__(self):
        for batch in self.dataloader:
            yield {
                k: v for k, v in batch.items() if k not in self.exclude_keys
            }

    def __len__(self):
        return len(self.dataloader)
# ...
# 后面用的时候

    #  构造 exclude list
    exclude_features = []
    if not cfg.use_depth_image:
        exclude_features += ["observation.images.side_depth", "observation.images.side_depth_is_pad"]
    if not cfg.use_force:
        exclude_features += ["observation.force", "observation.force_is_pad"]
    if not cfg.use_language_tip:
        # 加语言引导的
        pass

    #  包装 dataloader
    dataloader = FilteredBatchLoader(raw_dataloader, exclude_features)
    peek_batch = next(iter(dataloader))
    print("真正训练的时候甬道的feature：", list(peek_batch.keys()))

```
3. 为了使用本地模型训练, smolvla里面关于vlm_model_name的全改成本地目录
   ```python
   # vlm_model_name: str = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"  # Select the VLM backbone.
    # 直接把这个vlm_model_name改成local
    vlm_model_name:str="models/forsmolvla/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
   ```
    modeling_smolvla里面改了一个`self.language_tokenizer = AutoProcessor.from_pretrained(self.config.vlm_model_name, local_files_only=True).tokenizer`但是感觉这个local_files_only可有可无
4. train with baseline/depth都还好。用filterbatch类就行，但是depth不确定用哪个训练，可以是video截的，也可以是真的depth的2通道图，从训练/测试准确性讲肯定是后者一致……

### use_true_depth
1. class TrainPipelineConfig的一个实例属性
2. train的时候加在参数里面，`dataset = make_dataset(cfg,use_true_depth=cfg.use_true_depth)`
3. lerobotdataset初始化的时候加上这个属性
   1. `__getitem__` 用_query_videos获得图片
       1. 初始化的时候就新增 
        ```python
        # 为了深度,获得episode_index对应的frame index
        self.episode_start_indices = {}
        for idx in range(len(self.hf_dataset)):
            item = self.hf_dataset[idx]
            ep_idx = item["episode_index"].item()
            if ep_idx not in self.episode_start_indices:
                self.episode_start_indices[ep_idx] = idx
        ```
        2. 获得episode_index和frame_index用于定位图片
        ```python
        episode_start_idx = self.episode_start_indices[ep_idx]
        frame_idx = [
            i - episode_start_idx for i in query_indices['observation.images.side_depth']
        ]
        ``` 
        3. 调用`_query_videos`，传入frame_idx
   2. `_query_videos`这个函数：输入的是video episode，输出图片，调用一次只得到一张图
    ```python
        # 当use_true_depth且vid_key此时含有depth的时候,传入图片index,直接从本地得到图片
        if self.use_true_depth and "depth" in vid_key:
            frames = self.read_image_frames_from_disk(
                vid_key,
                ep_idx, 
                query_indices)
    ```
   3. `read_image_frames_from_index`从本地读图片，返回(0,1)的
    ```python
    # 从images文件夹里读取
    def read_image_frames_from_disk(
        self,
        cam_key: str,
        ep_idx: int,
        query_indices
    ) -> torch.Tensor:
        """
        Reads frames from images/{cam_key}/episode_{ep_idx} that match query_timestamps.
        Assumes one image per timestamp in order, named frame_000000.png, etc.

        Returns:
            torch.Tensor: Tensor of shape [N, 3, H, W], float32, in [0,1]
        """
        

        # 1. Image directory
        images_dir = self.root / "images" / cam_key / f"episode_{ep_idx:06d}"
        if not images_dir.exists():
            raise FileNotFoundError(f"Image directory does not exist: {images_dir}")

        # 2. Get episode timestamps from self.episode_data_index

        # 5. Load corresponding image files
        frames = []
        for i in query_indices:
            i = int(i)
            fname = images_dir / f"frame_{i:06d}.png"
            if not fname.exists():
                raise FileNotFoundError(f"Expected frame not found: {fname}")
            img = PIL.Image.open(fname).convert("RGB")
            img_np = np.array(img)                  # 转成 NumPy 数组，形状为 (H, W, 3)
            frames.append(TF.to_tensor(img))  # float32, [0, 1], shape [3, H, W]

        return torch.stack(frames)  # [N, 3, H, W]
    ``` 

### use_depth_image 
1. 也是train的参数，默认为false
2. 用exclude_features保留训练的时候需要用的feature
   ```python
    exclude_features = []
    if not cfg.use_depth_image:
        exclude_features += ["observation.images.side_depth", "observation.images.side_depth_is_pad"]
    if not cfg.use_force:
        exclude_features += ["observation.force", "observation.force_is_pad"]
   ```
3. `dataloader = FilteredBatchLoader(raw_dataloader, exclude_features, obj_detector=obj_detector)`得到最后训练时候的input feature（这个方法比较诡异，一是训练完的文件里面，input features是所有（实际上模型的input feature没那么多）；二是这个动作在下一个epoch的时候会重复执行）更好的方式是在larobotdatset的getitem那里就筛选？虽然也会重复……
    ```python
    class FilteredBatchLoader:
    def __init__(self, dataloader, exclude_keys: list, obj_detector=None, save_task_path='training_dataset/0727pickplace/first100/meta/modified_tasks.jsonl'):
        self.dataloader = dataloader
        self.exclude_keys = set(exclude_keys)
        self.obj_detector = obj_detector
        self.save_task_path = Path(save_task_path) if save_task_path else None
        if self.save_task_path:
            self.save_task_path.parent.mkdir(parents=True, exist_ok=True)
            self.task_f = open(self.save_task_path, "a")  # 以 append 模式写入
    
    def __del__(self):
        if hasattr(self, "task_f") and not self.task_f.closed:
            self.task_f.close()

    def __iter__(self):
        for batch in self.dataloader:
            filtered_batch = {k: v for k, v in batch.items() if k not in self.exclude_keys}
            yield filtered_batch
    ```


### use_language_tip
功能：在训练的时候task里面增加内容，告诉它目前的gripper位置和object的位置；

由于目前只能用单卡，训练的时候不方便加载物体识别模型，所以改成：先不训练，load所有data，然后保存task。训练的时候直接从文件里面读取task。

obj_detector_api分成三个

1. 物体识别+定位
   1. gripper采用yolo，物体直接用opencv颜色识别（其实物体也能yolo的，但颜色比较快）
   2. 定位求出center就够了
2. 坐标转换
   1. 先get2d，再通过realsense得到3d
   2. 用我想要的坐标系处在当前坐标下的位置，搞一个oxy，就能坐标转换
3. 写入task
   1. 外部传入color_img（用于物体识别）,depth_img（必须是无损的），task
   2. 输出加了坐标描述的task

```python
import math
import os
import sys
import cv2
import torch
import numpy as np
from PIL import Image
import pyrealsense2 as rs
from ultralytics import YOLO  # 导入YOLO库

# 手动构造 intrinsics 对象
intrinsics = rs.intrinsics()
intrinsics.width = 640
intrinsics.height = 480
intrinsics.ppx = 304.7939453125
intrinsics.ppy = 234.874755859375
intrinsics.fx = 616.6113891601562
intrinsics.fy = 616.5948486328125
intrinsics.model = rs.distortion.inverse_brown_conrady
intrinsics.coeffs = [0.0, 0.0, 0.0, 0.0, 0.0]
depth_scale = 0.0010000000474974513

class YOLOProcessor:
    def __init__(self, 
                 model_path="yolo/training/runs/train/yolo11nano_custom2/weights/best.pt",  # 使用微调后的YOLO模型
                 device="cuda" if torch.cuda.is_available() else "cpu"):
        self.device = device
        self.model = YOLO(model_path).to(self.device)  # 加载YOLO模型
        self.fail_counter = 0
        self.class_names = {0: "gripper"}  # 只需要夹子的类别定义
        self.conf_threshold = 0.25  # 置信度阈值
        
        # 淡黄色物体的HSV颜色范围
        # self.lower_yellow = np.array([20, 50, 100])
        # self.upper_yellow = np.array([35, 150, 255])
        self.lower_yellow = np.array([15, 30, 80])
        self.upper_yellow = np.array([45, 255, 255])
        self.min_contour_area = 100  # 最小轮廓面积
        self.total_images = 0
        self.gripper_detected = 0
        self.object_detected = 0


    def _transform_image(self, image_tensor):
        """将(0,1)范围的浮点张量转换为YOLO所需的预处理图像"""
        # 1. 转换到0-255范围并转换为uint8
        image_np = (image_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
        return image_np


    def _get_bbox_center(self, bbox):
        """
        给定bbox = [x1, y1, x2, y2]，返回中心点 (cx, cy)
        """
        x1, y1, x2, y2 = bbox
        cx = int((x1 + x2) / 2)
        cy = int((y1 + y2) / 2)
        return (cx, cy)


    def opencv_detect_yellow_object(self, color_image):
        """使用OpenCV检测淡黄色物体"""
        # 转换为HSV颜色空间
        hsv = cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)
        
        # 创建黄色物体的掩码
        mask = cv2.inRange(hsv, self.lower_yellow, self.upper_yellow)
        
        # 形态学操作去除噪声
        kernel = np.ones((5, 5), np.uint8)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        
        # 查找轮廓
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # 寻找面积最大的轮廓
        max_area = 0
        best_contour = None
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if area > self.min_contour_area and area > max_area:
                max_area = area
                best_contour = cnt
        
        # 计算边界框和中心点
        if best_contour is not None:
            x, y, w, h = cv2.boundingRect(best_contour)
            center_x = x + w // 2
            center_y = y + h // 2
            return (center_x, center_y), (x, y, w, h)
        
        return None, None

    # 相当于main,处理图像,得到坐标,加入task
    @torch.no_grad()
    def process_sample(self, side_img: torch.Tensor, side_depth: torch.Tensor):
        # 1. 图像预处理
        image_tensor = self._transform_image(side_img)
        depth_tensor=self._transform_image(side_depth)


        
        # # 2. YOLO推理 - 只检测夹子
        gripper_center = None
        object_center = None
        threed_pos = [None, None]
        
        # 检测夹子
        # torch.cuda.set_device(self.device)
        results = self.model.predict(
            image_tensor, 
            imgsz=640,
            conf=self.conf_threshold,
            device=self.device,
            verbose=False
        )
        # torch.cuda.set_device(self.device)
        # 3. 解析YOLO结果 - 只取夹子
        for result in results:
            if result.boxes is not None:
                for box in result.boxes:
                    # 只处理夹子类别
                    if int(box.cls.item()) == 0 and box.conf.item() >= self.conf_threshold:
                        gripper_box = box.xyxy[0].cpu().numpy()
                        gripper_center = self._get_bbox_center(gripper_box)
                        break  # 只取第一个检测到的夹子
        
        # # 4. 使用OpenCV检测淡黄色物体
        # # 注意：OpenCV需要BGR格式，但我们的图像是RGB，需要转换
        bgr_image = cv2.cvtColor(image_tensor, cv2.COLOR_RGB2BGR)
        object_center, object_bbox = self.opencv_detect_yellow_object(bgr_image)
        
        # # 5. 获取3D坐标
        centers = []
        if gripper_center is not None:
            centers.append(gripper_center)
        if object_center is not None:
            centers.append(object_center)
        
                # 统计逻辑
        self.total_images += 1
        if gripper_center is not None:
            self.gripper_detected += 1
        if object_center is not None:
            self.object_detected += 1

        
        # # 6. 转换为3D坐标
        if centers:
            threed_pos = self.pixel_to_3d(depth_tensor, centers)

        """
        测试center是否正确
        """

        # 仅调试用：可视化检测点并保存图像
        # if gripper_center is not None:
        #     cv2.circle(debug_vis, gripper_center, 8, (0, 255, 0), -1)  # green
        # if object_center is not None:
        #     cv2.circle(debug_vis, object_center, 8, (0, 0, 255), -1)  # red
        # debug_vis = depth_tensor.copy()
        # debug_save_path='outputs/depth'
        # cv2.imwrite(debug_save_path, depth_tensor)

        
        return threed_pos



    def transform_camera_to_custom_coordsystem(self,points_3d):
        """
        将一组相机坐标转换为自定义坐标系（以机械臂底座为原点，布面为XY平面）
        Args:
            points_3d: List of (x, y, z) in camera coordinate system
        Returns:
            List of transformed (x, y, z) in custom coordinate system
        """

        # 已知平均局部坐标系（三个单位向量 + 原点）
        # 这个坐标系要重新修改
        origin = np.array([ 0.24163092, -0.08227619,  0.60075652])
        x_axis = np.array([-0.36651895, -0.77909696,  0.50859786])
        y_axis = np.array([-0.92731940,  0.26136948, -0.26788937])
        z_axis = np.array([ 0.07577983, -0.56981920, -0.81826860])

        # 构建旋转矩阵（列向量为各轴）
        R = np.stack([x_axis, y_axis, z_axis], axis=1)  # shape (3,3)

        # 执行变换
        converted = []
        for p in points_3d:
            if p is None:
                converted.append(None)
            else:
                p = np.array(p)
                local_p = np.dot(R.T, p - origin)
                converted.append(tuple(local_p))

        return converted

    def get_average_sevenpoints_3d_coords(self, depth_batch):
        """
        给定一组彩色深度图，计算指定7个像素点在所有图像中的3D坐标均值。
        
        Args:
            depth_batch (List[np.ndarray]): 每张图是彩色编码深度图（H x W x 3）
        Returns:
            List[Tuple[float, float, float] or None]: 每个点的平均3D坐标
        """

        sevenpoints = [
            (557, 149), (633, 261), (612, 236),
            (594, 212), (614, 326), (576, 340), (538, 350)
        ]
        
        # 每个点的位置分别记录有效的3D坐标值
        collected_points = [[] for _ in range(len(sevenpoints))]

        for depth_img in depth_batch:
            # 对当前图像计算7个点的3D坐标
            depth_tensor=self._transform_image(depth_img)
            points_3d = self.pixel_to_3d(depth_tensor, sevenpoints)
            
            for i, p in enumerate(points_3d):
                if p is not None:
                    collected_points[i].append(np.array(p))

        # 对每个点求平均值
        avg_points = []
        for pts in collected_points:
            if len(pts) == 0:
                avg_points.append(None)
            else:
                mean = np.mean(pts, axis=0)
                avg_points.append(tuple(float(v) for v in mean))

        return avg_points

        
    def add_depth_info_to_task(self, rgb_batch, depth_batch, task_batch):


        rgb_batch = rgb_batch.to(self.device)
        depth_batch = depth_batch.to(self.device)
        """添加深度信息到任务描述"""
        updated_tasks = []
        
        for rgb, depth, task in zip(rgb_batch, depth_batch, task_batch):

            # 如果已经加了就跳过
            if "|" in task or "gripper at" in task or "Pyramid-Shaped Sachet at" in task:
                print("处理过了,pass")
                updated_tasks.append(task_str)
                continue

            # 获取两个目标的3D坐标(相机坐标系)
            points_3d = self.process_sample(rgb, depth)

        
            if points_3d and len(points_3d) >= 2:
                # 变成机械臂底座的坐标系,因为这个函数没有处理None,就放在这儿了
                converted_3d = self.transform_camera_to_custom_coordsystem(points_3d)
                a = converted_3d[0]  # gripper
                b = converted_3d[1]  # object
                
                # 如果坐标无效，使用占位符
                a_str = f"({a[0]:.3f}, {a[1]:.3f}, {a[2]:.3f})" if a is not None else "(N/A, N/A, N/A)"
                b_str = f"({b[0]:.3f}, {b[1]:.3f}, {b[2]:.3f})" if b is not None else "(N/A, N/A, N/A)"
                
                task_str = f"{task} | gripper at {a_str}m, the Pyramid-Shaped Sachet at {b_str}m"
            else:
                task_str = f"{task} |"
            
            updated_tasks.append(task_str)
        
        return updated_tasks



    def remove_edge_of_r_cahnnel(b_encoded):
        """
        输入:一个通道
        输出:给通道去边缘
        测试的成功率:(只有这一个通道),比较的对象是原图(原图->视频编码->提取图,输入是提取图)
        [Bilateral + medianBlur + 局部跳变补偿]
        最大误差: 100
        平均误差: 0.51
        误差为0的像素占比: 84.48%
        误差大于10的像素占比: 0.80%
        """
        # Step 1: Bilateral 滤波
        b_filtered = cv2.bilateralFilter(b_encoded, d=5, sigmaColor=75, sigmaSpace=75)

        # Step 2: Laplacian 边缘增强
        laplacian = cv2.Laplacian(b_filtered, cv2.CV_64F, ksize=1)
        b_restored = b_encoded - laplacian
        b_restored = np.clip(b_restored, 0, 255).astype(np.uint8)

        # Step 3: medianBlur
        b_median = cv2.medianBlur(b_restored, 3)

        # Step 4: Sobel 局部跳变补偿
        sobel_x = cv2.Sobel(b_median, cv2.CV_16S, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(b_median, cv2.CV_16S, 0, 1, ksize=3)
        gradient = cv2.convertScaleAbs(cv2.magnitude(sobel_x.astype(np.float32), sobel_y.astype(np.float32)))
        jump_mask = (gradient > 40)
        b_local_avg = cv2.blur(b_median, (3, 3))
        b_final = b_median.copy()
        b_final[jump_mask] = b_local_avg[jump_mask]
        return b_final
        

    # 无损返回深度值 根本不可能实现
    def decode_depth_from_rgb(self,rgb_image: np.ndarray) -> np.ndarray:
        # 此时的rgb_image是彩色深度图.用rg复原depth_uint16
        r = rgb_image[:, :, 0].astype(np.uint8)
        g = rgb_image[:, :, 1].astype(np.uint8)
        b = rgb_image[:, :, 2].astype(np.uint8)
        # 先给图片去边缘(大概把)
        depth_uint16 = ((r.astype(np.uint16) << 8) | g.astype(np.uint16))
        return depth_uint16

    def pixel_to_3d(self, depth_image, pixels, radius=5):
        """将像素坐标转换为3D坐标，支持以像素为中心、半径为radius的平均深度"""
        
        # 将彩色深度图转换为原始深度值
        oned_depth_image = self.decode_depth_from_rgb(depth_image)
        
        height, width = oned_depth_image.shape
        points_3d = []
        
        for (u, v) in pixels:
            # 检查中心点是否在图像范围内
            if v < 0 or v >= height or u < 0 or u >= width:
                self.fail_counter += 1
                print(f"{self.fail_counter}: bbox超出范围")
                points_3d.append(None)
                continue

            # 获取局部区域的深度值
            u_min = max(0, u - radius)
            u_max = min(width - 1, u + radius)
            v_min = max(0, v - radius)
            v_max = min(height - 1, v + radius)

            depth_patch = oned_depth_image[v_min:v_max+1, u_min:u_max+1]
            valid_depths = depth_patch[depth_patch > 0]

            if valid_depths.size == 0:
                points_3d.append(None)
                continue

            # 计算平均深度
            depth_raw = valid_depths.mean()
            depth_in_meters = depth_raw * depth_scale

            # 转换为3D坐标
            point = rs.rs2_deproject_pixel_to_point(intrinsics, [u, v], depth_in_meters)
            points_3d.append(tuple(point))  # (x, y, z)

        return points_3d

    def count_distance(self, rgb_batch, depth_batch):
        """计算两个目标之间的距离。支持单张图像或batch输入"""
        
        # 判断是否是单张图片（Tensor）
        # TODO:不确定是tensor还是numpy
        is_single = isinstance(rgb_batch, torch.Tensor) and rgb_batch.dim() == 3

        # 如果是单张图片，包装成列表
        if is_single:
            rgb_batch = [rgb_batch]
            depth_batch = [depth_batch]

        distances = []
        for rgb, depth in zip(rgb_batch, depth_batch):
            points_3d = self.process_sample(rgb, depth)
            
            if points_3d and len(points_3d) >= 2 and all(p is not None for p in points_3d[:2]):
                a = points_3d[0]
                b = points_3d[1]
                distance = math.sqrt((a[0] - b[0])**2 + 
                                    (a[1] - b[1])**2 + 
                                    (a[2] - b[2])**2)
                distances.append(distance)
            else:
                distances.append(None)

        # 返回单个值还是列表
        # 如果是单个，返回一个，else返回一个list
        return distances[0] if is_single else distances


    def print_statistics(self):
        """打印识别成功率统计"""
        if self.total_images == 0:
            print("尚未处理任何图像。")
            return

        gripper_rate = self.gripper_detected / self.total_images * 100
        object_rate = self.object_detected / self.total_images * 100

        print(f"总图像数: {self.total_images}")
        print(f"Gripper 检测成功率: {gripper_rate:.2f}% ({self.gripper_detected}/{self.total_images})")
        print(f"Object 检测成功率: {object_rate:.2f}% ({self.object_detected}/{self.total_images})")

```



#### 保存task
1. class FilteredBatchLoader
   ```python
   def __iter__(self):
        for batch in self.dataloader:
            filtered_batch = {k: v for k, v in batch.items() if k not in self.exclude_keys}

            if self.obj_detector is not None:
                images = filtered_batch["observation.images.side"]
                depths = filtered_batch["observation.images.side_depth"]
                tasks = filtered_batch["task"]

                new_tasks = self.obj_detector.add_depth_info_to_task(images, depths, tasks)
                filtered_batch["task"] = new_tasks

                #  保存每帧的修改后task
                if self.save_task_path:
                    ep_indices = filtered_batch["episode_index"]
                    frame_indices = filtered_batch["frame_index"]
                    for ep, frame, task in zip(ep_indices, frame_indices, new_tasks):
                        record = {
                            "episode_index": int(ep.item()),
                            "frame_index": int(frame.item()),
                            "task": task,
                        }
                        self.task_f.write(json.dumps(record) + "\n")

                self.obj_detector.print_statistics()

            yield filtered_batch
   ```
   同时要定义obj_detector
   ```python
       # if cfg.use_language_tip:
    #     from simplify_work.obj_dection.detector_api import YOLOProcessor
    #     obj_detector = YOLOProcessor()
   ```
2. 注释所有的policy.train相关的。run train.py
3. 另外，因为会不断循环，所以当看到episode99之后出现episode0的时候可以终止了,如果终止得太晚可以截取前多少行
4. 然后给task空的增加内容，我直接就近平均的
   ```python
   import json
    from pathlib import Path
    from tqdm import tqdm
    import re
    import numpy as np
    from collections import defaultdict

    INPUT_PATH = "training_dataset/0727pickplace/first100/meta/modified_tasks_truncated.jsonl"
    OUTPUT_PATH = "training_dataset/0727pickplace/first100/meta/modified_tasks_filled.jsonl"
    NEIGHBOR_RANGE = 3  # 前后各取 3 帧

    def parse_position(text, key):
        match = re.search(rf"{key} at \(([-\d.,\s]+)\)m", text)
        if match:
            return tuple(map(float, match.group(1).split(",")))
        return None

    def format_position(name, pos):
        return f"{name} at ({pos[0]:.3f}, {pos[1]:.3f}, {pos[2]:.3f})m"

    def fill_missing(episode_data):
        filled_data = []
        n = len(episode_data)

        # 提取 gripper 和 object 位置
        gripper_positions = [parse_position(d['task'], "gripper") for d in episode_data]
        object_positions = [parse_position(d['task'], "the Pyramid-Shaped Sachet") for d in episode_data]

        for i in range(n):
            g_pos = gripper_positions[i]
            o_pos = object_positions[i]
            new_task = episode_data[i]["task"]

            def avg_from_neighbors(pos_list):
                neighbors = []
                for offset in range(1, NEIGHBOR_RANGE + 1):
                    for j in [i - offset, i + offset]:
                        if 0 <= j < n and pos_list[j] is not None:
                            neighbors.append(pos_list[j])
                    if len(neighbors) >= 2:
                        break
                if neighbors:
                    return tuple(np.mean(neighbors, axis=0))
                return None

            # 如果缺失 gripper
            if g_pos is None:
                g_pos = avg_from_neighbors(gripper_positions)
                if g_pos:
                    new_task += " " + format_position("gripper", g_pos)
            # 如果缺失 object
            if o_pos is None:
                o_pos = avg_from_neighbors(object_positions)
                if o_pos:
                    if "gripper at" in new_task:
                        new_task += ", " + format_position("the Pyramid-Shaped Sachet", o_pos)
                    else:
                        new_task += " " + format_position("the Pyramid-Shaped Sachet", o_pos)

            filled_data.append({
                "episode_index": episode_data[i]["episode_index"],
                "frame_index": episode_data[i]["frame_index"],
                "task": new_task.strip()
            })

        return filled_data

    def main():
        path = Path(INPUT_PATH)
        with path.open("r") as f:
            lines = [json.loads(line) for line in f]

        episodes = defaultdict(list)
        for entry in lines:
            episodes[entry["episode_index"]].append(entry)

        output_lines = []
        for episode_index in tqdm(sorted(episodes), desc="Filling positions"):
            episode_data = sorted(episodes[episode_index], key=lambda x: x["frame_index"])
            filled = fill_missing(episode_data)
            output_lines.extend(filled)

        with open(OUTPUT_PATH, "w") as f:
            for entry in output_lines:
                f.write(json.dumps(entry) + "\n")

    if __name__ == "__main__":
        main()
   ```

#### read task
1. 在lerobotdataset的getitem，当train的时候会set use_true_depth为true。就会直接用修改后的task
   ```python
     # Add task as a string
        if self.use_true_depth:
            # 从本地读取
            if not len(frame_idx) == 1:
                raise KeyError("说好只读一个呢")
            frame_idx=frame_idx[0]
            if hasattr(self, "modified_tasks") and ep_idx in self.modified_tasks and frame_idx in self.modified_tasks[ep_idx]:
                item["task"] = self.modified_tasks[ep_idx][frame_idx]
            else:
                task_idx = item["task_index"].item()
                item["task"] = self.meta.tasks[task_idx]
        else:
            # 平常的task
            task_idx = item["task_index"].item()
            item["task"] = self.meta.tasks[task_idx]
    ```
2. 同时增加修改后的task的获取方式
   ```python
    def load_modified_tasks(path):
        """从 JSONL 文件中加载修改后的任务，返回 nested dict[ep_idx][frame_idx] = task"""
        from collections import defaultdict
        import json

        result = defaultdict(dict)
        with open(path, "r") as f:
            for line in f:
                item = json.loads(line)
                ep = item["episode_index"]
                frame = item["frame_index"]
                result[ep][frame] = item["task"]
        return result
   ```
   init的时候调用`self.modified_tasks = load_modified_tasks("training_dataset/0727pickplace/first100/meta/modified_tasks_filled.jsonl")`修改路径。


#### yolo
```python
# train_yolo11nano.py

from ultralytics import YOLO

# 加载YOLOv11-nano模型（确保 ultralytics 包支持该模型）
model = YOLO("yolo11n.pt")

# 开始训练
model.train(
    data="data.yaml",  # 数据集配置文件路径
    epochs=400,                   # 训练轮次
    imgsz=(640, 480),                    # 输入图像大小
    batch=16,                     # 批大小
    name="yolo11nano_custom",     # 实验名称/输出文件夹
    project="runs/train",         # 训练输出保存路径
    device=0                      # 使用GPU（0）或CPU（'cpu'）
)
```
数据标注是在roboflow搞的，很快





## structure
1. update_policy
   1. 真实训练过程
2. train
   1. 用trainpipelineconfig创建train过程，wandb没用了，检查device，load dataset和policy，optimizer和grad等
   2. 调用update_policy
   3. save checkpoint
3. 如果要继续训练的话就要用    if cfg.resume
    1. 命令行： `nohup env CUDA_VISIBLE_DEVICES=1 HF_HUB_OFFLINE=1 PYTHONPATH=src   python -m lerobot.scripts.train  --resume=true --config_path=outputs/train/0807/language_relative/checkpoints/last/pretrained_model/train_config.json  --steps=20000 > logs/0807/train_relative_resume.log 2>&1 &`
4. train的时候四个参数，默认都是false
   1. use_depth_image：为true：train的时候会用depth image作为input feature
   2. use_force：为true的时候用force，但是没写
   3. use_language_tip
      1. save task的时候：为true就表示要用task
      2. read task的时候，LeRobotDataset需要加这个参数，会从本地读取：顺序：train->make_dataset->LerobotDataset->getitem
   4. use_true_depth：为true就是make_dataset的时候用深度本地图片，也是对应lerobotdatse的getitem的处理。

# accelerate-train
还没添加。可以参考https://github.com/huggingface/lerobot/pull/1246

# evaluate
## usage
1. 服务器运行 `CUDA_VISIBLE_DEVICES=3 python simplify_work/server/server_code/get_data_from_client.py` cuda_visible可以不加
2. 本地运行 `ENV=local HF_HUB_OFFLINE=1 PYTHONPATH="src:simplify_work" python -m lerobot.record --config_path=simplify_work/work/eavluate/evaluate_0713_first50.yaml --policy.path=outputs/train/0709_first50/checkpoints/last/pretrained_model` 第一个是用来区分本地和服务器的。

## modified
1. modeling_smolvla.py

区分服务器和本地。本地就调用predict_from_server_api，服务器则是直接用policy.get_action_chunk
```python
IS_LOCAL = os.environ.get("ENV", "") == "local"
print(f"IS_LOCAL = {IS_LOCAL}")
# 如果判断是本地的
if IS_LOCAL:
    # 服务器推理
    import sys

    # 获取该文件所在目录（control_utils.py）
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 找到 simplify_work/server/local_code 所在目录的绝对路径
    predict_code_dir = os.path.abspath(os.path.join(current_dir, '../../../../../simplify_work/server/local_code'))

    # 临时加入 Python 模块搜索路径
    if predict_code_dir not in sys.path:
        sys.path.insert(0, predict_code_dir)
    from predict_from_server_api import predict_from_server
```
```python
# 开启服务器推理。本地使用这个，并且注释掉后面四行。
        if IS_LOCAL:
            actions=predict_from_server(batch)
        else:
        # 服务器就注释掉上面一行，使用下面四行。
            images, img_masks = self.prepare_images(batch)
            state = self.prepare_state(batch)
            lang_tokens, lang_masks = self.prepare_language(batch)
            actions = self.model.sample_actions(images, img_masks, lang_tokens, lang_masks, state, noise=noise)

```


## structure
1. record.py 一开始调用的，DatasetRecordConfig，RecordConfig，record_loop和record。调用predict_action

```python
action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
```

2. control_utils里面的predict_action

```python
action = policy.select_action(observation)
```

### send_action
Q：软件层面怎么做的，send_action的action从哪来，到哪去

record.py -> record函数
来
```python
        elif policy is None and isinstance(teleop, list):
            # TODO(pepijn, steven): clean the record loop for use of multiple robots (possibly with pipeline)
            arm_action = teleop_arm.get_action()
            arm_action = {f"arm_{k}": v for k, v in arm_action.items()}

            keyboard_action = teleop_keyboard.get_action()
            base_action = robot._from_keyboard_to_base_action(keyboard_action)

            action = {**arm_action, **base_action} if len(base_action) > 0 else arm_action
```

去：
```python
sent_action = robot.send_action(action)

# 整个流程有一个控制的 30fps

if dataset is not None:
    action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
    frame = {**observation_frame, **action_frame}
    dataset.add_frame(frame, task=single_task)

if display_data:
    log_rerun_data(observation, action)

dt_s = time.perf_counter() - start_loop_t
busy_wait(1 / fps - dt_s)

timestamp = time.perf_counter() - start_episode_t
```

推理的时候，dt_s包含get_observation,get_action,send_action这三个步骤的时间。当get_action时间比较长的时候，busy_wait就要等一个负数，会直接跳过，不然就是用1/fps当作每次send_action之间的


robot.send_action(action) 发送action，robot是so100 follower，此时self.bus是

```python
self.bus = FeetechMotorsBus(
            port=self.config.port,
            motors={
                "shoulder_pan": Motor(1, "sts3215", norm_mode_body),
                "shoulder_lift": Motor(2, "sts3215", norm_mode_body),
                "elbow_flex": Motor(3, "sts3215", norm_mode_body),
                "wrist_flex": Motor(4, "sts3215", norm_mode_body),
                "wrist_roll": Motor(5, "sts3215", norm_mode_body),
                "gripper": Motor(6, "sts3215", MotorNormMode.RANGE_0_100),
            },
            calibration=self.calibration,
        )
```
FeetchMotorBus继承MotorsBus类

send_action函数：

```python
 def send_action(self, action: dict[str, Any]) -> dict[str, Any]:
        """Command arm to move to a target joint configuration.

        The relative action magnitude may be clipped depending on the configuration parameter
        `max_relative_target`. In this case, the action sent differs from original action.
        Thus, this function always returns the action actually sent.

        Raises:
            RobotDeviceNotConnectedError: if robot is not connected.

        Returns:
            the action sent to the motors, potentially clipped.
        """
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        goal_pos = {key.removesuffix(".pos"): val for key, val in action.items() if key.endswith(".pos")}

        # Cap goal position when too far away from present position.
        # /!\ Slower fps expected due to reading from the follower.
        if self.config.max_relative_target is not None:
            present_pos = self.bus.sync_read("Present_Position")
            goal_present_pos = {key: (g_pos, present_pos[key]) for key, g_pos in goal_pos.items()}
            goal_pos = ensure_safe_goal_position(goal_present_pos, self.config.max_relative_target)

        # Send goal position to the arm
        self.bus.sync_write("Goal_Position", goal_pos)
        return {f"{motor}.pos": val for motor, val in goal_pos.items()}
```
把goal position写入bus。

因为FeetechMotorsBus没有overwrite sync_write函数，所以实际上执行的是MotorsBus里面定义的sync_write函数
self.bus的类FeetechMotorsBus里面没有sync_write这个函数，sync_write实际是在class MotorsBus(abc.ABC):这个类里面定义的函数。怎么用的？写入到哪里？写入之后怎么变成机械臂的动作的？什么速度？->这些也没法得知了



# async evaluate 
## usgae
### server
理的时候虽然可以不加CUDA_VISIBLE_DEVICES但是如果要的话就加在服务器端
```
CUDA_VISIBLE_DEVICES=3 python -m lerobot.scripts.server.policy_server \
    --host="10.10.1.35" \
    --port=9000
```

如果报错说`google.protobuf.runtime_version.VersionError: Detected incompatible Protobuf Gencode/Runtime versions when loading lerobot/transport/services.proto: gencode 6.31.0 runtime 6.30.2. Runtime version cannot be older than the linked gencode version. See Protobuf version guarantees at https://protobuf.dev/support/cross-version-runtime-guarantee.`：
pip install grpcio-tools
python -m grpc_tools.protoc -I src --python_out=src --grpc_python_out=src src/lerobot/transport/services.proto
没有报错就能运行了
正常的话可以看到cfg ok
INFO 2025-07-28 07:18:12 y_server.py:384 {'fps': 30,
 'host': '10.10.1.35',
 'inference_latency': 0.03333333333333333,
 'obs_queue_timeout': 2,
 'port': 9000}
INFO 2025-07-28 07:18:12 y_server.py:394 PolicyServer started on 10.10.1.35:9000

### client
本地需要有模型的input feature文件，可以把整个pretrained_model文件夹搞下来，但不用safetensors

```
CUDA_VISIBLE_DEVICES=1 HF_HUB_OFFLINE=1 PYTHONPATH=src python src/lerobot/scripts/server/robot_client.py \
    --server_address=10.10.1.35:9000 \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem585A0076841 \
    --robot.id=follower_so100 \
    --robot.cameras="{ laptop: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}, phone: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}}" \
    --task="pick up the pyramid-shaped sachet and place it into the box." \
    --policy_type=smolvla \
    --pretrained_name_or_path=outputs/train/0727/depth_100/checkpoints/004000/pretrained_model \
    --policy_device=mps \
    --actions_per_chunk=50 \
    --chunk_size_threshold=0.5 \
    --aggregate_fn_name=weighted_average \
    --debug_visualize_queue_size=True
```
简单点:

HF_HUB_OFFLINE=1 PYTHONPATH=src python src/lerobot/scripts/server/robot_client.py --config_path=simplify_work/eva_wih_async/local_template.yaml

## structure
### main
1. 启动服务器，监听
2. 客户端发送model config->服务器接收并加载模型
3. 初始：客户端发送第一个时间点的observation，设置mustgo为true。服务器接收到之后推理出chunksize大小的action chunk和distance，发给客户端。
4. 客户端判断distance的距离，选择avaraged_action，并删除queue里面的若干个action。发送给robot执行。
5. 对其他observation，客户端发送，服务器接收后判断要不要用，如果要的话就推理出action chunk，继续发送给客户端。客户端进行筛选，保留queue的准确性。当不在写queue的时候，提取queue的内容并发送给robot

### policy_server

### robot_clinet
1. 连接机器人设备（通过串口）
2. 与远程服务器建立gRPC通信
3. 获取摄像头数据，打包发送为 observation
4. 接收服务器生成的动作（action）指令
5. 根据距离动态选择要平均的动作数量，然后控制机器人执行动作
6. 在后台不断进行观测发送和动作执行（control loop）


## distance控制action执行多少步
1. 先不用distance了，直接用opencv做也够了



# 贴颜色块之后重新采集，更改language
objdetectorapi，原来load yolo model之后计算位置，改成直接读取颜色。


# 推理用language引导
客户端robot_clinet 服务端 policy_server

## 代码结构
1. 客户端一个config：use_language_tip
   1. RobotClientConfig 增加property，command里可以写入
   2. robot发送给server，server作为一个inner property：modify_task
2. 发送：RemotePolicyConfig
   1. RemotePolicyConfig 增加modify_task: bool = False
   2. self.policy_config = RemotePolicyConfig构造的时候补全
3. server
   1. 接收modify_task ：def SendPolicyInstructions(self, request, context):
   2. 推理之前处理好task
   
## 多样化language
自动化流程

0. 设计prompt
1. 根据modified_tasks生成新的prompt 存储为jsonl文件
2. 读取jsonl文件，传入训练流程
3. 测试的时候用prompt方式


# 开始加新的玩意儿
把结构加到state后边儿
会极大地影响结果，为什么？