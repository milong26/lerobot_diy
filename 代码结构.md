> 我再也不搞merge了因为auto-merge真的把我当傻子

# install1
1. 直接按照lerobot教程install
2. cuda环境和pytorch环境最好一致，后来因为找不到cuda12.0用的pytorch所以改成pytorch11.9+cuda11.8，反正都是安装




# collect
1. port一般没动，需要`sudo chmod 666 /dev/ttyACM0` 0，1，2都要sudo，2是力传感器
2. 找opencv相机 `PYTHONPATH=src python -m lerobot.find_cameras` 一般也不用改，opencv不是6就是7，再找不到才用这个

## usage
先改好代码，或者直接用这一版的代码。

1. 收集：yaml文件提前写好 `PYTHONPATH=src python -m lerobot.record --config_path=simplify_work/work/collect/collect_data_template.yaml`
2. visualize一下数据集：`PYTHONPATH=src  python -m lerobot.scripts.visualize_dataset --repo-id=haello/1 --root=/home/qwe/.cache/huggingface/lerobot/test/collect_1 --episode-index=0`

如果要继续加一个 --resume=True

上传到服务器 `rsync -avz  fcam1/all150/ user@xx.xx.xx.xx:location in server`

## modified

有个小bug，没法读电机，改so100_follower_end_effector，新增num_retry=3`obs_dict = self.bus.sync_read("Present_Position",num_retry=3)`

### realsense camera
depth转深度图并保存

1. camera_pyrealsense.py 收集 需要有一个可以读color_image和depth_image的函数

    ```python

    def async_read_combined(self, timeout_ms: float = 200) -> tuple[np.ndarray, np.ndarray]:
        """
        Asynchronously reads the latest synchronized color + depth frame.

        This method retrieves the most recent frames captured by the background
        read thread. It does not block on hardware but waits for the latest frame.

        Args:
            timeout_ms (float): Max time in milliseconds to wait for a new frame.

        Returns:
            Tuple[np.ndarray, np.ndarray]: (color_image, depth_image)

        Raises:
            DeviceNotConnectedError: If the camera is not connected.
            TimeoutError: If no frame becomes available within the timeout.
            RuntimeError: If internal state is inconsistent.
        """
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        if self.thread is None or not self.thread.is_alive():
            self._start_read_thread()

        if not self.new_frame_event.wait(timeout=timeout_ms / 1000.0):
            raise TimeoutError(f"Timed out waiting for frame from {self}")

        with self.frame_lock:
            color = self.latest_frame
            self.new_frame_event.clear()

        with self.depth_frame_lock:
            depth = self.latest_depth_frame
            self.new_depth_frame_event.clear()
        if self.use_depth:
            if color is None or depth is None :
                raise RuntimeError(f"{self}: Frame data incomplete")
        if self.use_depth:
            return color, depth
        else:
            return color, None
    ```
2. so100_follower.py 设备端
   1. make_force_sensor
   2. 启动、采集、关闭
3. force feature :dataset/utils
4. visualize_utils： 遇到observation.force暂时不绘制
5. record的时候给传感器归零处理
6. record程序里面需要导入！就算不用也要导入


### force
1. 新增了一个repo
2. types里面增加FeatureType force
3. utils里面force的feature

# train

## usage
1. 纯服务器调用：上传数据集后，在服务器端运行`nohup env CUDA_VISIBLE_DEVICES=1 HF_HUB_OFFLINE=1 PYTHONPATH=src   python -m lerobot.scripts.train   --config_path=simplify_work/work/train/fine_tune_0713_first100_depth.yaml   --policy.path=models/forsmolvla/smolvla_base   --policy.repo_id=lerobot/smolvla_base   > train_100_depth.log 2>&1 &`
2. 查看日至 `tail -f log.file`
3. 参数 configs/train.py里面，加了三个
    
    ```python
    # 为了在多模态里面选择用/不用depth_image，默认为false。scripts/train.py
    use_depth_image: bool=False
    use_force: bool=False
    use_language_tip: bool=False
    ```
4. 本地小测试`HF_HUB_OFFLINE=1 PYTHONPATH=src   python -m lerobot.scripts.train   --config_path=simplify_work/work/train/local_test.yaml   --policy.path=models/forsmolvla/smolvla_base   --policy.repo_id=lerobot/smolvla_base`

## modified
scripts/train.py里面

1. 修改tokenizer和multi gpu bug,os的指定改到命令行里面了

```python
# https://github.com/huggingface/lerobot/issues/1377
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
```
2. 根据config的3个变量，filter batch的内容，因为原始数据集里面feature比较多

```python
class FilteredBatchLoader:
    def __init__(self, dataloader, exclude_keys: list):
        self.dataloader = dataloader
        self.exclude_keys = set(exclude_keys)

    def __iter__(self):
        for batch in self.dataloader:
            yield {
                k: v for k, v in batch.items() if k not in self.exclude_keys
            }

    def __len__(self):
        return len(self.dataloader)
# ...
# 后面用的时候

    #  构造 exclude list
    exclude_features = []
    if not cfg.use_depth_image:
        exclude_features += ["observation.images.side_depth", "observation.images.side_depth_is_pad"]
    if not cfg.use_force:
        exclude_features += ["observation.force", "observation.force_is_pad"]
    if not cfg.use_language_tip:
        # 加语言引导的
        pass

    #  包装 dataloader
    dataloader = FilteredBatchLoader(raw_dataloader, exclude_features)
    peek_batch = next(iter(dataloader))
    print("真正训练的时候甬道的feature：", list(peek_batch.keys()))

```
3. 为了使用本地模型训练, smolvla里面关于vlm_model_name的全改成本地目录
   ```python
   # vlm_model_name: str = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"  # Select the VLM backbone.
    # 直接把这个vlm_model_name改成local
    vlm_model_name:str="models/forsmolvla/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
   ```
    modeling_smolvla里面改了一个`self.language_tokenizer = AutoProcessor.from_pretrained(self.config.vlm_model_name, local_files_only=True).tokenizer`但是感觉这个local_files_only可有可无


## structure
1. update_policy
   1. 真实训练过程
2. train
   1. 用trainpipelineconfig创建train过程，wandb没用了，检查device，load dataset和policy，optimizer和grad等
   2. 调用update_policy
   3. save checkpoint
3. 如果要继续训练的话就要用    if cfg.resume:


# train with instruction
## usage
1. 下载模型 
2. cd simplify_work/obj_dection/GroundingDINO/  pip install -e .
2. 先注释掉load policy相关的,`CUDA_VISIBLE_DEVICES=2 HF_HUB_OFFLINE=1 PYTHONPATH=src   python -m lerobot.scripts.train   --policy.path=models/forsmolvla/smolvla_base   --policy.repo_id=lerobot/smolvla_base --config_path=simplify_work/work/train/use_language.yaml`
3. 第一遍最好别断网,因为有个bert需要下载,但我不知道应该放在哪里


## object detection
~~选择yoloe:real-time seeing anything  wget https://huggingface.co/jameslahm/yoloe/resolve/main/yoloe-v8l-seg.pt 也可以走镜像：wget https://hf-mirror.com/jameslahm/yoloe/resolve/main/yoloe-v8l-seg.pt ~~

改用dino ground，比yolo好用

1. obj_dection里面readme下载
2. simplify_work/obj_dection/detector_api.py version1集成api，输入图像位置，输出phase/2d位置都可以

```python
import os
import sys
import cv2

# 添加 GroundingDINO 到 sys.path
current_dir = os.path.dirname(os.path.abspath(__file__))
groundingdino_path = os.path.join(current_dir, "GroundingDINO")
sys.path.append(groundingdino_path)

from groundingdino.util.inference import load_model, load_image, predict, annotate

# 模型配置
CONFIG_PATH = "simplify_work/obj_dection/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
CHECKPOINT_PATH = "models/objdection/dinoground/groundingdino_swint_ogc.pth"
DEVICE = "cpu"

# 用一个全局变量保存模型
_model = None

def get_model():
    global _model
    if _model is None:
        print("[INFO] Loading GroundingDINO model...")
        _model = load_model(CONFIG_PATH, CHECKPOINT_PATH)
    return _model

def detect_image(image_path, text_prompt, box_thresh=0.25, text_thresh=0.25):
    image_source, image = load_image(image_path)
    model = get_model()
    boxes, logits, phrases = predict(
        model=model,
        image=image,
        caption=text_prompt,
        box_threshold=box_thresh,
        text_threshold=text_thresh,
        device=DEVICE,
    )
    annotated = annotate(image_source, boxes, logits, phrases)
    return boxes, phrases, annotated

```

3. lerobot train的时候image是从video得到的，格式是numpy/tensor ，改写detector_api.py v2
   1. 输入image() 和 对应的depth_image
   2. 返回obj的3d坐标
4. 新增了关于筛选同类别分数最高的obj,得到彩色图的2d坐标(图片的原点是左上角,向右x增大,向下y增大)
5. rs需要从1channel的深度图(值)获得,但之前的深度值经过归一化已经变成3channel了,要先变成1channel
   1. 之前realsense是怎么归一化深度的?有没有坐标缩放?
api
```python

```

trian
```
```


# accelerate-train
还没添加。可以参考https://github.com/huggingface/lerobot/pull/1246

# evaluate
## usage
1. 服务器运行 `CUDA_VISIBLE_DEVICES=3 python simplify_work/server/server_code/get_data_from_client.py` cuda_visible可以不加
2. 本地运行 `ENV=local HF_HUB_OFFLINE=1 PYTHONPATH="src:simplify_work" python -m lerobot.record --config_path=simplify_work/work/eavluate/evaluate_0713_first50.yaml --policy.path=outputs/train/0709_first50/checkpoints/last/pretrained_model` 第一个是用来区分本地和服务器的。

## modified
1. modeling_smolvla.py

区分服务器和本地。本地就调用predict_from_server_api，服务器则是直接用policy.get_action_chunk
```python
IS_LOCAL = os.environ.get("ENV", "") == "local"
print(f"IS_LOCAL = {IS_LOCAL}")
# 如果判断是本地的
if IS_LOCAL:
    # 服务器推理
    import sys

    # 获取该文件所在目录（control_utils.py）
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 找到 simplify_work/server/local_code 所在目录的绝对路径
    predict_code_dir = os.path.abspath(os.path.join(current_dir, '../../../../../simplify_work/server/local_code'))

    # 临时加入 Python 模块搜索路径
    if predict_code_dir not in sys.path:
        sys.path.insert(0, predict_code_dir)
    from predict_from_server_api import predict_from_server
```
```python
# 开启服务器推理。本地使用这个，并且注释掉后面四行。
        if IS_LOCAL:
            actions=predict_from_server(batch)
        else:
        # 服务器就注释掉上面一行，使用下面四行。

            images, img_masks = self.prepare_images(batch)
            state = self.prepare_state(batch)
            lang_tokens, lang_masks = self.prepare_language(batch)
            actions = self.model.sample_actions(images, img_masks, lang_tokens, lang_masks, state, noise=noise)

```


## structure
1. record.py 一开始调用的，DatasetRecordConfig，RecordConfig，record_loop和record。调用predict_action

```python
action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
```

2. control_utils里面的predict_action

```python
action = policy.select_action(observation)
```

### send_action
Q：软件层面怎么做的，send_action的action从哪来，到哪去

record.py -> record函数
来
```python
        elif policy is None and isinstance(teleop, list):
            # TODO(pepijn, steven): clean the record loop for use of multiple robots (possibly with pipeline)
            arm_action = teleop_arm.get_action()
            arm_action = {f"arm_{k}": v for k, v in arm_action.items()}

            keyboard_action = teleop_keyboard.get_action()
            base_action = robot._from_keyboard_to_base_action(keyboard_action)

            action = {**arm_action, **base_action} if len(base_action) > 0 else arm_action
```

去：
```python
sent_action = robot.send_action(action)

# 整个流程有一个控制的 30fps

if dataset is not None:
    action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
    frame = {**observation_frame, **action_frame}
    dataset.add_frame(frame, task=single_task)

if display_data:
    log_rerun_data(observation, action)

dt_s = time.perf_counter() - start_loop_t
busy_wait(1 / fps - dt_s)

timestamp = time.perf_counter() - start_episode_t
```

推理的时候，dt_s包含get_observation,get_action,send_action这三个步骤的时间。当get_action时间比较长的时候，busy_wait就要等一个负数，会直接跳过，不然就是用1/fps当作每次send_action之间的


robot.send_action(action) 发送action，robot是so100 follower，此时self.bus是

```python
self.bus = FeetechMotorsBus(
            port=self.config.port,
            motors={
                "shoulder_pan": Motor(1, "sts3215", norm_mode_body),
                "shoulder_lift": Motor(2, "sts3215", norm_mode_body),
                "elbow_flex": Motor(3, "sts3215", norm_mode_body),
                "wrist_flex": Motor(4, "sts3215", norm_mode_body),
                "wrist_roll": Motor(5, "sts3215", norm_mode_body),
                "gripper": Motor(6, "sts3215", MotorNormMode.RANGE_0_100),
            },
            calibration=self.calibration,
        )
```
FeetchMotorBus继承MotorsBus类

send_action函数：

```python
 def send_action(self, action: dict[str, Any]) -> dict[str, Any]:
        """Command arm to move to a target joint configuration.

        The relative action magnitude may be clipped depending on the configuration parameter
        `max_relative_target`. In this case, the action sent differs from original action.
        Thus, this function always returns the action actually sent.

        Raises:
            RobotDeviceNotConnectedError: if robot is not connected.

        Returns:
            the action sent to the motors, potentially clipped.
        """
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        goal_pos = {key.removesuffix(".pos"): val for key, val in action.items() if key.endswith(".pos")}

        # Cap goal position when too far away from present position.
        # /!\ Slower fps expected due to reading from the follower.
        if self.config.max_relative_target is not None:
            present_pos = self.bus.sync_read("Present_Position")
            goal_present_pos = {key: (g_pos, present_pos[key]) for key, g_pos in goal_pos.items()}
            goal_pos = ensure_safe_goal_position(goal_present_pos, self.config.max_relative_target)

        # Send goal position to the arm
        self.bus.sync_write("Goal_Position", goal_pos)
        return {f"{motor}.pos": val for motor, val in goal_pos.items()}
```
把goal position写入bus。

因为FeetechMotorsBus没有overwrite sync_write函数，所以实际上执行的是MotorsBus里面定义的sync_write函数
self.bus的类FeetechMotorsBus里面没有sync_write这个函数，sync_write实际是在class MotorsBus(abc.ABC):这个类里面定义的函数。怎么用的？写入到哪里？写入之后怎么变成机械臂的动作的？什么速度？->这些也没法得知了



# async evaluate 
1. 服务器
```
python -m lerobot.scripts.server.policy_server \
    --host="10.10.1.35" \
    --port=9000
```

如果报错说`google.protobuf.runtime_version.VersionError: Detected incompatible Protobuf Gencode/Runtime versions when loading lerobot/transport/services.proto: gencode 6.31.0 runtime 6.30.2. Runtime version cannot be older than the linked gencode version. See Protobuf version guarantees at https://protobuf.dev/support/cross-version-runtime-guarantee.`
pip install grpcio-tools
python -m grpc_tools.protoc -I src --python_out=src --grpc_python_out=src src/lerobot/transport/services.proto
没有报错就能运行了
正常的话可以看到cfg ok
INFO 2025-07-28 07:18:12 y_server.py:384 {'fps': 30,
 'host': '10.10.1.35',
 'inference_latency': 0.03333333333333333,
 'obs_queue_timeout': 2,
 'port': 9000}
INFO 2025-07-28 07:18:12 y_server.py:394 PolicyServer started on 10.10.1.35:9000

2. 客户端
```
CUDA_VISIBLE_DEVICES=1 HF_HUB_OFFLINE=1 PYTHONPATH=src python src/lerobot/scripts/server/robot_client.py \
    --server_address=10.10.1.35:9000 \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem585A0076841 \
    --robot.id=follower_so100 \
    --robot.cameras="{ laptop: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}, phone: {type: opencv, index_or_path: 0, width: 1920, height: 1080, fps: 30}}" \
    --task="pick up the pyramid-shaped sachet and place it into the box." \
    --policy_type=smolvla \
    --pretrained_name_or_path=outputs/train/0727/depth_100/checkpoints/004000/pretrained_model \
    --policy_device=mps \
    --actions_per_chunk=50 \
    --chunk_size_threshold=0.5 \
    --aggregate_fn_name=weighted_average \
    --debug_visualize_queue_size=True

```

CUDA_VISIBLE_DEVICES=1 HF_HUB_OFFLINE=1 PYTHONPATH=src python src/lerobot/scripts/server/robot_client.py --config_path=simplify_work/eva_wih_async/local_template.yaml


实际运行的时候报错path不对，所以本地还要搞模型的配置文件。不过我也好奇前面的设置有没有用

rsync -av --exclude='*.safetensors' user@remote_host:/xxx/lerobot_diy/outputs/ /home/qwe/wokonsmall/lerobot_diy/outputs/
下载，不下载.safetensors
因为代码（config.json有点问题，所以要手动删config.json里面的一些input featrue）

两边都能运行了但是没看到action，看看服务器能不能输出action，本地能不能接收到

发现好像我保存的depth有点问题...
不会要重新训练吧.....

get_observation,这个时候得到的应该就是蓝色的,返回dict给record,observation_frame=build_dataset_frame,这个没有改value

然后到dataset.add_frame(frame, task=single_task)






        img_dir = self.root / "images"
        if img_dir.is_dir():
            shutil.rmtree(self.root / "images")
这个函数用来删除图片文件夹


为什么lerobot得到的深度图是蓝色的
so100follower.py
                color, depth = cam.async_read_combined()
                obs_dict[cam_key]=color
                if depth is not None:
                    obs_dict[cam_key+"_depth"]=depth

调用camera realsense的async_read_combined

        img_dir = self.root / "images"
        if img_dir.is_dir():
            shutil.rmtree(self.root / "images")
这个函数用来删除图片文件夹


# 坐标系转换
得到的深度图也是RGB格式

去掉边缘:;lerobot收集的时候图片->视频->提取图片得到的结果和图片有差异,主要集中在边缘部分.但我想复原

# tools
懒得整理,先放
从视频中得到图片并保存为png
```python

    """
    保存图片到本地
    """

    # 创建保存目录
    os.makedirs("outputs/foryolo", exist_ok=True)
    import numpy as np

    img_counter = 0
    for batch_idx, batch in enumerate(raw_dataloader):
        if "observation.images.side" in batch:
            images = batch["observation.images.side"]  # [B, C, H, W]
            
            for i in range(images.shape[0]):
                img_tensor = images[i].detach().cpu()  # [C, H, W]
                
                # 1. 转换为NumPy数组并调整通道顺序 (C,H,W) -> (H,W,C)
                if img_tensor.shape[0] == 3:  # RGB图像
                    img_np = img_tensor.permute(1, 2, 0).numpy()
                else:  # 灰度图像或特殊情况
                    img_np = img_tensor.squeeze(0).numpy()  # [H, W]
                
                # 2. 检查并修复异常形状 (1,1,3)
                if img_np.shape == (1, 1, 3):
                    # 尝试恢复为原始尺寸（根据您的数据集实际情况修改尺寸）
                    original_height, original_width = 224, 224  # 示例尺寸，需替换为真实值
                    try:
                        img_np = img_np.reshape(original_height, original_width, 3)
                    except ValueError:
                        logging.warning(f"无法重塑图像 {img_counter}，跳过")
                        continue
                
                
                # 4. 转换为uint8（PIL要求的数据类型）
                img_np = img_np.astype(np.uint8)
                
                # 5. 处理单通道灰度图
                if img_np.ndim == 2 or (img_np.ndim == 3 and img_np.shape[2] == 1):
                    img_np = img_np.squeeze()  # 移除单通道维度
                
                # 6. 保存为PNG
                try:
                    img_path = os.path.join("outputs/foryolo", f"{img_counter:06d}.png")
                    Image.fromarray(img_np).save(img_path)
                    img_counter += 1
                except Exception as e:
                    logging.error(f"保存图像失败: {e}, 形状: {img_np.shape}, 类型: {img_np.dtype}")
                    continue
                    
        # 每10批次打印一次进度
        if batch_idx % 10 == 0:
            logging.info(f"已保存 {img_counter} 张图像...")
    raise KeyError("yolo准备工作")

```


dataloader做了什么,对视频(图像)
因为我打算保存图片发现make_dataset得到的都是[0,1]这个范围内的.所以dataloader怎么从食品得到图片的
dataset=make_dataset`



        img_dir = self.root / "images"
        if img_dir.is_dir():
            shutil.rmtree(self.root / "images")
这个函数用来删除图片文件夹


为什么lerobot得到的深度图是蓝色的
so100follower.py
                color, depth = cam.async_read_combined()
                obs_dict[cam_key]=color
                if depth is not None:
                    obs_dict[cam_key+"_depth"]=depth

调用camera realsense的async_read_combined


# 坐标系转换
得到的深度图也是RGB格式

去掉边缘:;lerobot收集的时候图片->视频->提取图片得到的结果和图片有差异,主要集中在边缘部分.但我想复原



# 复原不了
1. 不删除图片文件
```python
    def encode_episode_videos(self, episode_index: int) -> None:
            ...
            encode_video_frames(img_dir, video_path, self.fps, overwrite=True)
            # shutil.rmtree(img_dir) 这一行是删除图片文件夹的
```

2. read video的时候怎么变成图片的:替换视频解码为直接从 images/ 文件夹中读取图像帧
还是lerobotdataset这个类
调用
```python
 def _query_videos(self, query_timestamps: dict[str, list[float]], ep_idx: int) -> dict[str, torch.Tensor]:
        item = {}
        for vid_key, query_ts in query_timestamps.items():
            # 当use_real_depth且vid_key此时含有depth的时候
            if self.use_real_depth and "depth" in vid_key:
                frames = self.read_image_frames_from_disk(vid_key, ep_idx, query_ts)
```

在接口里面,读照片并传入,