> 我再也不搞merge了因为auto-merge真的把我当傻子

# collect
## usage
PYTHONPATH=src python -m lerobot.record --config_path=simplify_work/work/collect/collect_data_template.yaml

visualize一下数据集：`PYTHONPATH=src  python -m lerobot.scripts.visualize_dataset --repo-id=haello/1 --root=/home/qwe/.cache/huggingface/lerobot/test/collect_1 --episode-index=0`

## modified

有个小bug，没法读电机，改so100_follower_end_effector，新增num_retry=3`obs_dict = self.bus.sync_read("Present_Position",num_retry=3)`

### realsense camera

depth转深度图并保存

1. camera_pyrealsense.py 收集 需要有一个可以读color_image和depth_image的函数

    ```python

    def async_read_combined(self, timeout_ms: float = 200) -> tuple[np.ndarray, np.ndarray]:
        """
        Asynchronously reads the latest synchronized color + depth frame.

        This method retrieves the most recent frames captured by the background
        read thread. It does not block on hardware but waits for the latest frame.

        Args:
            timeout_ms (float): Max time in milliseconds to wait for a new frame.

        Returns:
            Tuple[np.ndarray, np.ndarray]: (color_image, depth_image)

        Raises:
            DeviceNotConnectedError: If the camera is not connected.
            TimeoutError: If no frame becomes available within the timeout.
            RuntimeError: If internal state is inconsistent.
        """
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        if self.thread is None or not self.thread.is_alive():
            self._start_read_thread()

        if not self.new_frame_event.wait(timeout=timeout_ms / 1000.0):
            raise TimeoutError(f"Timed out waiting for frame from {self}")

        with self.frame_lock:
            color = self.latest_frame
            self.new_frame_event.clear()

        with self.depth_frame_lock:
            depth = self.latest_depth_frame
            self.new_depth_frame_event.clear()
        if self.use_depth:
            if color is None or depth is None :
                raise RuntimeError(f"{self}: Frame data incomplete")
        if self.use_depth:
            return color, depth
        else:
            return color, None

    ```
2. so100_follower.py 设备端
   1. make_force_sensor
   2. 启动、采集、关闭
3. force feature :dataset/utils
4. visualize_utils： 遇到observation.force暂时不绘制
5. record的时候给传感器归零处理
6. record程序里面需要导入！就算不用也要导入


### force
1. 新增了一个repo
2. types里面增加FeatureType force
3. utils里面force的feature

# train

## usage
1. 纯服务器调用：上传数据集后，在服务器端运行`CUDA_VISIBLE_DEVICES=2 HF_HUB_OFFLINE=1 python lerobot/scripts/train.py --config_path=simplify_work/work/train/xxx.yaml`
2. 参数 configs/train.py里面，加了三个
    ```python
    """
    为了在多模态里面选择用/不用depth_image，默认为false。scripts/train.py
    """
    use_depth_image: bool=False
    use_force: bool=False
    use_language_tip: bool=False
    ```

## modified
scripts/train.py里面

1. 修改tokenizer和multi gpu bug,os的指定改到命令行里面了

```python
# https://github.com/huggingface/lerobot/issues/1377
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
```
2. 根据config的3个变量，filter batch的内容，因为原始数据集里面feature比较多

```python
class FilteredBatchLoader:
    def __init__(self, dataloader, exclude_keys: list):
        self.dataloader = dataloader
        self.exclude_keys = set(exclude_keys)

    def __iter__(self):
        for batch in self.dataloader:
            yield {
                k: v for k, v in batch.items() if k not in self.exclude_keys
            }

    def __len__(self):
        return len(self.dataloader)
# ...
# 后面用的时候

    #  构造 exclude list
    exclude_features = []
    if not cfg.use_depth_image:
        exclude_features += ["observation.images.side_depth", "observation.images.side_depth_is_pad"]
    if not cfg.use_force:
        exclude_features += ["observation.force", "observation.force_is_pad"]
    if not cfg.use_language_tip:
        # 加语言引导的
        pass

    #  包装 dataloader
    dataloader = FilteredBatchLoader(raw_dataloader, exclude_features)
    peek_batch = next(iter(dataloader))
    print("真正训练的时候甬道的feature：", list(peek_batch.keys()))

```
3. 为了使用本地模型训练, smolvla里面关于vlm_model_name的全改成本地目录
   ```python
   # vlm_model_name: str = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"  # Select the VLM backbone.
    # 直接把这个vlm_model_name改成local
    vlm_model_name:str="models/forsmolvla/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
   ```
    modeling_smolvla里面改了一个`self.language_tokenizer = AutoProcessor.from_pretrained(self.config.vlm_model_name, local_files_only=True).tokenizer`但是感觉这个local_files_only可有可无


## structure
1. update_policy
   1. 真实训练过程
2. train
   1. 用trainpipelineconfig创建train过程，wandb没用了，检查device，load dataset和policy，optimizer和grad等
   2. 调用update_policy
   3. save checkpoint
3. 如果要继续训练的话就要用    if cfg.resume:


# accelerate-train
还没添加。可以参考https://github.com/huggingface/lerobot/pull/1246

# evaluate
## usage
1. 服务器运行 `CUDA_VISIBLE_DEVICES=3 python simplify_work/server/server_code/get_data_from_client.py` cuda_visible可以不加
2. 本地运行 `ENV=local python -m lerobot.record --config_path=simplify_work/work/evaluate_record.yaml` 第一个是用来区分本地和服务器的。

## modified
1. modeling_smolvla.py

区分服务器和本地。本地就调用predict_from_server_api，服务器则是直接用policy.get_action_chunk
```python
IS_LOCAL = os.environ.get("ENV", "") == "local"
print(f"IS_LOCAL = {IS_LOCAL}")
# 如果判断是本地的
if IS_LOCAL:
    # 服务器推理
    import sys

    # 获取该文件所在目录（control_utils.py）
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 找到 simplify_work/server/local_code 所在目录的绝对路径
    predict_code_dir = os.path.abspath(os.path.join(current_dir, '../../../../../simplify_work/server/local_code'))

    # 临时加入 Python 模块搜索路径
    if predict_code_dir not in sys.path:
        sys.path.insert(0, predict_code_dir)
    from predict_from_server_api import predict_from_server
```
```python
# 开启服务器推理。本地使用这个，并且注释掉后面四行。
        if IS_LOCAL:
            actions=predict_from_server(batch)
        else:
        # 服务器就注释掉上面一行，使用下面四行。

            images, img_masks = self.prepare_images(batch)
            state = self.prepare_state(batch)
            lang_tokens, lang_masks = self.prepare_language(batch)
            actions = self.model.sample_actions(images, img_masks, lang_tokens, lang_masks, state, noise=noise)

```


## structure
1. record.py 一开始调用的，DatasetRecordConfig，RecordConfig，record_loop和record。调用predict_action

```python
action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
```

2. control_utils里面的predict_action

```python
action = policy.select_action(observation)
```
