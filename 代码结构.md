> 我再也不搞merge了因为auto-merge真的把我当傻子

# collect
## usage
PYTHONPATH=src python -m lerobot.record --config_path=simplify_work/work/collect/collect_data_template.yaml

visualize一下数据集：`PYTHONPATH=src  python -m lerobot.scripts.visualize_dataset --repo-id=haello/1 --root=/home/qwe/.cache/huggingface/lerobot/test/collect_1 --episode-index=0`

## modified

有个小bug，没法读电机，改so100_follower_end_effector，新增num_retry=3`obs_dict = self.bus.sync_read("Present_Position",num_retry=3)`

### realsense camera
depth转深度图并保存

1. camera_pyrealsense.py 收集 需要有一个可以读color_image和depth_image的函数

    ```python

    def async_read_combined(self, timeout_ms: float = 200) -> tuple[np.ndarray, np.ndarray]:
        """
        Asynchronously reads the latest synchronized color + depth frame.

        This method retrieves the most recent frames captured by the background
        read thread. It does not block on hardware but waits for the latest frame.

        Args:
            timeout_ms (float): Max time in milliseconds to wait for a new frame.

        Returns:
            Tuple[np.ndarray, np.ndarray]: (color_image, depth_image)

        Raises:
            DeviceNotConnectedError: If the camera is not connected.
            TimeoutError: If no frame becomes available within the timeout.
            RuntimeError: If internal state is inconsistent.
        """
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        if self.thread is None or not self.thread.is_alive():
            self._start_read_thread()

        if not self.new_frame_event.wait(timeout=timeout_ms / 1000.0):
            raise TimeoutError(f"Timed out waiting for frame from {self}")

        with self.frame_lock:
            color = self.latest_frame
            self.new_frame_event.clear()

        with self.depth_frame_lock:
            depth = self.latest_depth_frame
            self.new_depth_frame_event.clear()
        if self.use_depth:
            if color is None or depth is None :
                raise RuntimeError(f"{self}: Frame data incomplete")
        if self.use_depth:
            return color, depth
        else:
            return color, None
    ```
2. so100_follower.py 设备端
   1. make_force_sensor
   2. 启动、采集、关闭
3. force feature :dataset/utils
4. visualize_utils： 遇到observation.force暂时不绘制
5. record的时候给传感器归零处理
6. record程序里面需要导入！就算不用也要导入


### force
1. 新增了一个repo
2. types里面增加FeatureType force
3. utils里面force的feature

# train

## usage
1. 纯服务器调用：上传数据集后，在服务器端运行`CUDA_VISIBLE_DEVICES=2 HF_HUB_OFFLINE=1 python lerobot/scripts/train.py --config_path=simplify_work/work/train/xxx.yaml`
2. 参数 configs/train.py里面，加了三个
    
    ```python
    # 为了在多模态里面选择用/不用depth_image，默认为false。scripts/train.py
    use_depth_image: bool=False
    use_force: bool=False
    use_language_tip: bool=False
    ```

## modified
scripts/train.py里面

1. 修改tokenizer和multi gpu bug,os的指定改到命令行里面了

```python
# https://github.com/huggingface/lerobot/issues/1377
import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
```
2. 根据config的3个变量，filter batch的内容，因为原始数据集里面feature比较多

```python
class FilteredBatchLoader:
    def __init__(self, dataloader, exclude_keys: list):
        self.dataloader = dataloader
        self.exclude_keys = set(exclude_keys)

    def __iter__(self):
        for batch in self.dataloader:
            yield {
                k: v for k, v in batch.items() if k not in self.exclude_keys
            }

    def __len__(self):
        return len(self.dataloader)
# ...
# 后面用的时候

    #  构造 exclude list
    exclude_features = []
    if not cfg.use_depth_image:
        exclude_features += ["observation.images.side_depth", "observation.images.side_depth_is_pad"]
    if not cfg.use_force:
        exclude_features += ["observation.force", "observation.force_is_pad"]
    if not cfg.use_language_tip:
        # 加语言引导的
        pass

    #  包装 dataloader
    dataloader = FilteredBatchLoader(raw_dataloader, exclude_features)
    peek_batch = next(iter(dataloader))
    print("真正训练的时候甬道的feature：", list(peek_batch.keys()))

```
3. 为了使用本地模型训练, smolvla里面关于vlm_model_name的全改成本地目录
   ```python
   # vlm_model_name: str = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct"  # Select the VLM backbone.
    # 直接把这个vlm_model_name改成local
    vlm_model_name:str="models/forsmolvla/HuggingFaceTB/SmolVLM2-500M-Video-Instruct"
   ```
    modeling_smolvla里面改了一个`self.language_tokenizer = AutoProcessor.from_pretrained(self.config.vlm_model_name, local_files_only=True).tokenizer`但是感觉这个local_files_only可有可无


## structure
1. update_policy
   1. 真实训练过程
2. train
   1. 用trainpipelineconfig创建train过程，wandb没用了，检查device，load dataset和policy，optimizer和grad等
   2. 调用update_policy
   3. save checkpoint
3. 如果要继续训练的话就要用    if cfg.resume:


# accelerate-train
还没添加。可以参考https://github.com/huggingface/lerobot/pull/1246

# evaluate
## usage
1. 服务器运行 `CUDA_VISIBLE_DEVICES=3 python simplify_work/server/server_code/get_data_from_client.py` cuda_visible可以不加
2. 本地运行 `ENV=local python -m lerobot.record --config_path=simplify_work/work/evaluate_record.yaml` 第一个是用来区分本地和服务器的。

## modified
1. modeling_smolvla.py

区分服务器和本地。本地就调用predict_from_server_api，服务器则是直接用policy.get_action_chunk
```python
IS_LOCAL = os.environ.get("ENV", "") == "local"
print(f"IS_LOCAL = {IS_LOCAL}")
# 如果判断是本地的
if IS_LOCAL:
    # 服务器推理
    import sys

    # 获取该文件所在目录（control_utils.py）
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 找到 simplify_work/server/local_code 所在目录的绝对路径
    predict_code_dir = os.path.abspath(os.path.join(current_dir, '../../../../../simplify_work/server/local_code'))

    # 临时加入 Python 模块搜索路径
    if predict_code_dir not in sys.path:
        sys.path.insert(0, predict_code_dir)
    from predict_from_server_api import predict_from_server
```
```python
# 开启服务器推理。本地使用这个，并且注释掉后面四行。
        if IS_LOCAL:
            actions=predict_from_server(batch)
        else:
        # 服务器就注释掉上面一行，使用下面四行。

            images, img_masks = self.prepare_images(batch)
            state = self.prepare_state(batch)
            lang_tokens, lang_masks = self.prepare_language(batch)
            actions = self.model.sample_actions(images, img_masks, lang_tokens, lang_masks, state, noise=noise)

```


## structure
1. record.py 一开始调用的，DatasetRecordConfig，RecordConfig，record_loop和record。调用predict_action

```python
action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
```

2. control_utils里面的predict_action

```python
action = policy.select_action(observation)
```

### send_action
Q：软件层面怎么做的，send_action的action从哪来，到哪去

record.py -> record函数
来
```python
        elif policy is None and isinstance(teleop, list):
            # TODO(pepijn, steven): clean the record loop for use of multiple robots (possibly with pipeline)
            arm_action = teleop_arm.get_action()
            arm_action = {f"arm_{k}": v for k, v in arm_action.items()}

            keyboard_action = teleop_keyboard.get_action()
            base_action = robot._from_keyboard_to_base_action(keyboard_action)

            action = {**arm_action, **base_action} if len(base_action) > 0 else arm_action
```

去：
```python
sent_action = robot.send_action(action)

# 整个流程有一个控制的 30fps

if dataset is not None:
    action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
    frame = {**observation_frame, **action_frame}
    dataset.add_frame(frame, task=single_task)

if display_data:
    log_rerun_data(observation, action)

dt_s = time.perf_counter() - start_loop_t
busy_wait(1 / fps - dt_s)

timestamp = time.perf_counter() - start_episode_t
```

推理的时候，dt_s包含get_observation,get_action,send_action这三个步骤的时间。当get_action时间比较长的时候，busy_wait就要等一个负数，会直接跳过，不然就是用1/fps当作每次send_action之间的


robot.send_action(action) 发送action，robot是so100 follower，此时self.bus是

```python
self.bus = FeetechMotorsBus(
            port=self.config.port,
            motors={
                "shoulder_pan": Motor(1, "sts3215", norm_mode_body),
                "shoulder_lift": Motor(2, "sts3215", norm_mode_body),
                "elbow_flex": Motor(3, "sts3215", norm_mode_body),
                "wrist_flex": Motor(4, "sts3215", norm_mode_body),
                "wrist_roll": Motor(5, "sts3215", norm_mode_body),
                "gripper": Motor(6, "sts3215", MotorNormMode.RANGE_0_100),
            },
            calibration=self.calibration,
        )
```
FeetchMotorBus继承MotorsBus类

send_action函数：

```python
 def send_action(self, action: dict[str, Any]) -> dict[str, Any]:
        """Command arm to move to a target joint configuration.

        The relative action magnitude may be clipped depending on the configuration parameter
        `max_relative_target`. In this case, the action sent differs from original action.
        Thus, this function always returns the action actually sent.

        Raises:
            RobotDeviceNotConnectedError: if robot is not connected.

        Returns:
            the action sent to the motors, potentially clipped.
        """
        if not self.is_connected:
            raise DeviceNotConnectedError(f"{self} is not connected.")

        goal_pos = {key.removesuffix(".pos"): val for key, val in action.items() if key.endswith(".pos")}

        # Cap goal position when too far away from present position.
        # /!\ Slower fps expected due to reading from the follower.
        if self.config.max_relative_target is not None:
            present_pos = self.bus.sync_read("Present_Position")
            goal_present_pos = {key: (g_pos, present_pos[key]) for key, g_pos in goal_pos.items()}
            goal_pos = ensure_safe_goal_position(goal_present_pos, self.config.max_relative_target)

        # Send goal position to the arm
        self.bus.sync_write("Goal_Position", goal_pos)
        return {f"{motor}.pos": val for motor, val in goal_pos.items()}
```
把goal position写入bus。

因为FeetechMotorsBus没有overwrite sync_write函数，所以实际上执行的是MotorsBus里面定义的sync_write函数
self.bus的类FeetechMotorsBus里面没有sync_write这个函数，sync_write实际是在class MotorsBus(abc.ABC):这个类里面定义的函数。怎么用的？写入到哪里？写入之后怎么变成机械臂的动作的？什么速度？->这些也没法得知了